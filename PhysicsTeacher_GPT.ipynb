{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install PyPDF2\n",
        "! pip install chromadb\n",
        "! pip install tavily"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uAqnrBsRqOdU",
        "outputId": "fc4f0eb0-5db8-4bce-8fe0-12fc0019fe5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.29.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Collecting urllib3<2.4.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-1.3.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.3.0-py3-none-any.whl (23 kB)\n",
            "Downloading kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.2.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m103.3/103.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.2-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.7.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (517 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.22.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=3239b483bf7318ea01773fd362a64dd7aee1cd8eb801b7ecad7ddff2cfa6bb4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, durationpy, uvloop, urllib3, pyproject_hooks, pybase64, opentelemetry-proto, mmh3, humanfriendly, httptools, bcrypt, backoff, watchfiles, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, build, posthog, opentelemetry-semantic-conventions, onnxruntime, opentelemetry-sdk, kubernetes, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "  Attempting uninstall: opentelemetry-proto\n",
            "    Found existing installation: opentelemetry-proto 1.37.0\n",
            "    Uninstalling opentelemetry-proto-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-proto-1.37.0\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 build-1.3.0 chromadb-1.3.5 coloredlogs-15.0.1 durationpy-0.10 httptools-0.7.1 humanfriendly-10.0 kubernetes-34.1.0 mmh3-5.2.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 pybase64-1.4.2 pypika-0.48.9 pyproject_hooks-1.2.0 urllib3-2.3.0 uvloop-0.22.1 watchfiles-1.1.1\n",
            "Collecting tavily\n",
            "  Downloading tavily-1.1.0.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tavily) (2.32.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from tavily) (3.13.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->tavily) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->tavily) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->tavily) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->tavily) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->tavily) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->tavily) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->tavily) (1.22.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tavily) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->tavily) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tavily) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->tavily) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->tavily) (4.15.0)\n",
            "Building wheels for collected packages: tavily\n",
            "  Building wheel for tavily (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tavily: filename=tavily-1.1.0-py3-none-any.whl size=6128 sha256=8f5d3f2214055de1f0bfe39761e72f85a6f3b74e267abe530327aa87cda13018\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/67/b7/9aec4851724de28ac2bc34ff10b042af43d7f2dd1552e5906e\n",
            "Successfully built tavily\n",
            "Installing collected packages: tavily\n",
            "Successfully installed tavily-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2k6RGXNUoTZf"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import gradio as gr\n",
        "import os\n",
        "import PyPDF2\n",
        "import chromadb\n",
        "from dotenv import load_dotenv\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"\" #Add your OPENAI API Key\n",
        "load_dotenv()\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"\")"
      ],
      "metadata": {
        "id": "4-RJl90aqRK1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Tavily client with error handling\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
        "try:\n",
        "    from tavily import TavilyClient\n",
        "    tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
        "    TAVILY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TAVILY_AVAILABLE = False\n",
        "    print(\"âš ï¸  Tavily not installed. Web search functionality will be disabled.\")\n",
        "    print(\"ğŸ’¡ Run: pip install tavily-python\")"
      ],
      "metadata": {
        "id": "CPfPUHSzquDh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ChromaDB\n",
        "chroma_client = chromadb.Client()\n",
        "pdf_collection = chroma_client.create_collection(name=\"PhysicsPDFs\")\n",
        "memory_collection = chroma_client.create_collection(name=\"ConversationMemory\")"
      ],
      "metadata": {
        "id": "lNVPtxo_qwko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "collapsed": true,
        "outputId": "a90d8d75-f62b-415d-d65f-87d866e414fc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "Collection [PhysicsPDFs] already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4077325177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize ChromaDB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mchroma_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchromadb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpdf_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchroma_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PhysicsPDFs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmemory_collection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchroma_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ConversationMemory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/client.py\u001b[0m in \u001b[0;36mcreate_collection\u001b[0;34m(self, name, schema, configuration, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconfiguration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"embedding_function\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         model = self._server.create_collection(\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/chromadb/api/rust.py\u001b[0m in \u001b[0;36mcreate_collection\u001b[0;34m(self, name, schema, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mschema_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         collection = self.bindings.create_collection(\n\u001b[0m\u001b[1;32m    243\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0mconfiguration_json_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Collection [PhysicsPDFs] already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "oBOB-Or_q2iR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=1000):\n",
        "    \"\"\"Split text into chunks\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "P3RXAKPqq55N"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_pdf_in_db(pdf_path):\n",
        "    \"\"\"Extract, chunk and store PDF in ChromaDB\"\"\"\n",
        "    try:\n",
        "        text = extract_text_from_pdf(pdf_path)\n",
        "        chunks = chunk_text(text)\n",
        "\n",
        "        # Clear existing data\n",
        "        pdf_collection.delete(where={\"source\": {\"$ne\": \"\"}})  # Delete all documents\n",
        "\n",
        "        # Add chunks to ChromaDB\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            pdf_collection.add(\n",
        "                documents=[chunk],\n",
        "                metadatas=[{\"source\": pdf_path, \"chunk_id\": i}],\n",
        "                ids=[f\"chunk_{i}_{os.path.basename(pdf_path)}\"]\n",
        "            )\n",
        "        return f\"âœ… Successfully loaded {len(chunks)} chunks from {pdf_path}\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {str(e)}\""
      ],
      "metadata": {
        "id": "-zm5JRQUq7eE"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_context(query, n_results=3):\n",
        "    \"\"\"Retrieve relevant context from ChromaDB\"\"\"\n",
        "    try:\n",
        "        results = pdf_collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=n_results\n",
        "        )\n",
        "        return \"\\n\\n\".join(results['documents'][0]) if results['documents'] else \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"ChromaDB query error: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "nDXOc2XCq-Bn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_conversation_memory(user_message, bot_response, conversation_context=\"\"):\n",
        "    \"\"\"Store conversation in long-term memory\"\"\"\n",
        "    try:\n",
        "        # Create a memory entry\n",
        "        memory_id = str(uuid.uuid4())\n",
        "        timestamp = datetime.now().isoformat()\n",
        "\n",
        "        # Combine user message and bot response for semantic search\n",
        "        memory_text = f\"User: {user_message}\\nBot: {bot_response}\"\n",
        "\n",
        "        if conversation_context:\n",
        "            memory_text = f\"Context: {conversation_context}\\n{memory_text}\"\n",
        "\n",
        "        # Store in memory collection\n",
        "        memory_collection.add(\n",
        "            documents=[memory_text],\n",
        "            metadatas=[{\n",
        "                \"type\": \"conversation\",\n",
        "                \"timestamp\": timestamp,\n",
        "                \"user_message\": user_message,\n",
        "                \"bot_response\": bot_response,\n",
        "                \"conversation_context\": conversation_context\n",
        "            }],\n",
        "            ids=[memory_id]\n",
        "        )\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Memory storage error: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "KfSlFGQByn1G"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_relevant_memories(query, n_results=2):\n",
        "    \"\"\"Retrieve relevant past conversations from memory\"\"\"\n",
        "    try:\n",
        "        results = memory_collection.query(\n",
        "            query_texts=[query],\n",
        "            n_results=n_results\n",
        "        )\n",
        "\n",
        "        memories = []\n",
        "        if results['documents']:\n",
        "            for i, doc in enumerate(results['documents'][0]):\n",
        "                metadata = results['metadatas'][0][i]\n",
        "                memory_text = f\"Previous conversation ({metadata['timestamp'][:10]}): {doc}\"\n",
        "                memories.append(memory_text)\n",
        "\n",
        "        return \"\\n\\n\".join(memories) if memories else \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Memory retrieval error: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "F4oC-8_qzQKO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_conversation_summary(history):\n",
        "    \"\"\"Generate a summary of the current conversation context\"\"\"\n",
        "    if not history:\n",
        "        return \"\"\n",
        "\n",
        "    # Take last few exchanges for context\n",
        "    recent_history = history[-3:] if len(history) > 3 else history\n",
        "    context_parts = []\n",
        "\n",
        "    for exchange in recent_history:\n",
        "        if isinstance(exchange, tuple) and len(exchange) == 2:\n",
        "            user_msg, bot_msg = exchange\n",
        "            context_parts.append(f\"User: {user_msg}\")\n",
        "            context_parts.append(f\"Bot: {bot_msg}\")\n",
        "        elif isinstance(exchange, list) and len(exchange) == 2:\n",
        "            user_msg, bot_msg = exchange\n",
        "            context_parts.append(f\"User: {user_msg}\")\n",
        "            context_parts.append(f\"Bot: {bot_msg}\")\n",
        "\n",
        "    return \"\\n\".join(context_parts)"
      ],
      "metadata": {
        "id": "z9yr1AOdyulc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_pdf_context_relevant(pdf_context, query):\n",
        "    \"\"\"Check if PDF context is actually relevant to the query\"\"\"\n",
        "    if not pdf_context or len(pdf_context.strip()) < 50:\n",
        "        return False\n",
        "\n",
        "    # Check for common irrelevant patterns in PDF context\n",
        "    irrelevant_patterns = [\n",
        "        \"table of contents\", \"copyright\", \"abstract\", \"references\",\n",
        "        \"appendix\", \"index\", \"acknowledg\", \"preface\"\n",
        "    ]\n",
        "\n",
        "    pdf_lower = pdf_context.lower()\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    # If PDF context contains many irrelevant sections, it's likely not useful\n",
        "    irrelevant_count = sum(1 for pattern in irrelevant_patterns if pattern in pdf_lower)\n",
        "    if irrelevant_count > 2:\n",
        "        return False\n",
        "\n",
        "    # Check if query terms actually appear in the context\n",
        "    query_terms = query_lower.split()\n",
        "    relevant_terms = sum(1 for term in query_terms if len(term) > 3 and term in pdf_lower)\n",
        "\n",
        "    # If less than 30% of meaningful query terms are in context, consider it irrelevant\n",
        "    meaningful_terms = [term for term in query_terms if len(term) > 3]\n",
        "    if meaningful_terms and (relevant_terms / len(meaningful_terms)) < 0.3:\n",
        "        return False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "id": "B-dSs5KkrEY5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_with_tavily(query):\n",
        "    \"\"\"Search using Tavily and get content from first result\"\"\"\n",
        "    if not TAVILY_AVAILABLE:\n",
        "        return \"Web search unavailable. Please install tavily-python: pip install tavily-python\"\n",
        "\n",
        "    try:\n",
        "        # Search for the query\n",
        "        search_response = tavily_client.search(\n",
        "            query=query,\n",
        "            search_depth=\"basic\",\n",
        "            max_results=3\n",
        "        )\n",
        "\n",
        "        if not search_response.get('results'):\n",
        "            return \"No search results found.\"\n",
        "\n",
        "        # Get the first result\n",
        "        first_result = search_response['results'][0]\n",
        "        first_url = first_result.get('url', '')\n",
        "\n",
        "        # Use Tavily's content if available\n",
        "        content = first_result.get('content', '')\n",
        "\n",
        "        if not content:\n",
        "            content = f\"Title: {first_result.get('title', 'N/A')}\\nURL: {first_url}\"\n",
        "\n",
        "        # Check if it's a PDF\n",
        "        if first_url.lower().endswith('.pdf'):\n",
        "            return f\"ğŸ“„ PDF Source: {first_url}\\n\\nThis appears to be a PDF document. Please visit the link to view the PDF: {first_url}\"\n",
        "\n",
        "        return f\"ğŸ” Search Result from: {first_url}\\n\\n{content}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Search error: {str(e)}\""
      ],
      "metadata": {
        "id": "oCpxqZxerIN5"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_physics_bot(message, history):\n",
        "    # Get relevant context from different sources\n",
        "    pdf_context = get_relevant_context(message)\n",
        "    memory_context = get_relevant_memories(message)\n",
        "    conversation_context = get_conversation_summary(history)\n",
        "\n",
        "    # Check if PDF context is actually relevant\n",
        "    use_pdf = is_pdf_context_relevant(pdf_context, message)\n",
        "\n",
        "    system_prompt = \"\"\"You are a friendly Physics Teacher Bot with memory. Follow these rules:\n",
        "    1. Use the provided context from PDFs, past conversations, and search results to answer questions accurately\n",
        "    2. If the context doesn't fully answer the question, use your knowledge to provide a complete answer\n",
        "    3. Reference past conversations when relevant to provide continuity\n",
        "    4. Be educational and clear in your explanations\n",
        "    5. Maintain context from the current conversation flow\"\"\"\n",
        "\n",
        "    # Build context sections\n",
        "    context_sections = []\n",
        "    source_notes = []\n",
        "\n",
        "    if use_pdf:\n",
        "        context_sections.append(f\"PDF Knowledge:\\n{pdf_context}\")\n",
        "        source_notes.append(\"ğŸ“š PDF\")\n",
        "\n",
        "    if memory_context:\n",
        "        context_sections.append(f\"Past Conversations:\\n{memory_context}\")\n",
        "        source_notes.append(\"ğŸ’­ Memory\")\n",
        "\n",
        "    if conversation_context:\n",
        "        context_sections.append(f\"Current Conversation:\\n{conversation_context}\")\n",
        "\n",
        "    # If no PDF context is relevant, use web search\n",
        "    if not use_pdf:\n",
        "        search_results = search_with_tavily(message)\n",
        "        context_sections.append(f\"Web Search:\\n{search_results}\")\n",
        "        source_notes.append(\"ğŸ” Web\")\n",
        "\n",
        "    # Combine all contexts\n",
        "    combined_context = \"\\n\\n\".join(context_sections) if context_sections else \"No specific context available.\"\n",
        "    source_note = \" + \".join(source_notes) if source_notes else \"General knowledge\"\n",
        "\n",
        "    user_content = f\"\"\"Available Context:\n",
        "{combined_context}\n",
        "\n",
        "Question: {message}\n",
        "\n",
        "Please provide a helpful answer based on the available context above.\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_content}\n",
        "            ],\n",
        "            temperature=0.2\n",
        "        )\n",
        "\n",
        "        bot_response = response.choices[0].message.content\n",
        "\n",
        "        # Store this conversation in long-term memory\n",
        "        store_conversation_memory(message, bot_response, conversation_context)\n",
        "\n",
        "        return f\"ğŸ”® Sources: {source_note}\\n\\n{bot_response}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error generating response: {str(e)}\""
      ],
      "metadata": {
        "id": "_HsNsC0JrNYX"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_memory():\n",
        "    \"\"\"Clear all conversation memory\"\"\"\n",
        "    try:\n",
        "        memory_collection.delete(where={\"type\": {\"$ne\": \"\"}})\n",
        "        return \"âœ… Conversation memory cleared successfully!\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error clearing memory: {str(e)}\""
      ],
      "metadata": {
        "id": "bojVEzNJzcql"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_memory():\n",
        "    \"\"\"Export conversation memory as JSON\"\"\"\n",
        "    try:\n",
        "        results = memory_collection.get()\n",
        "        memories = []\n",
        "\n",
        "        for i, doc in enumerate(results['documents']):\n",
        "            memories.append({\n",
        "                \"content\": doc,\n",
        "                \"metadata\": results['metadatas'][i],\n",
        "                \"id\": results['ids'][i]\n",
        "            })\n",
        "\n",
        "        filename = f\"conversation_memory_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        with open(filename, 'w') as f:\n",
        "            json.dump(memories, f, indent=2)\n",
        "\n",
        "        return f\"âœ… Memory exported to {filename}\"\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error exporting memory: {str(e)}\""
      ],
      "metadata": {
        "id": "k5T1f6fhzdpv"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf_and_chat(pdf_file, message, history):\n",
        "    # First store the PDF\n",
        "    result_msg = store_pdf_in_db(pdf_file.name)\n",
        "\n",
        "    # Then get response\n",
        "    response = chat_with_physics_bot(message, history)\n",
        "\n",
        "    return result_msg, response"
      ],
      "metadata": {
        "id": "nMkzyTidrWwn"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def launch_app():\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(\"# âš›ï¸ Physics Teacher Chat with PDF RAG + Web Search + Memory\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"])\n",
        "                load_btn = gr.Button(\"Load PDF into Knowledge Base\")\n",
        "                load_status = gr.Textbox(label=\"Load Status\", interactive=False)\n",
        "\n",
        "                gr.Markdown(\"### Memory Management\")\n",
        "                with gr.Row():\n",
        "                    clear_mem_btn = gr.Button(\"ğŸ§¹ Clear Memory\")\n",
        "                    export_mem_btn = gr.Button(\"ğŸ’¾ Export Memory\")\n",
        "                mem_status = gr.Textbox(label=\"Memory Status\", interactive=False)\n",
        "\n",
        "                gr.Markdown(\"\"\"\n",
        "                ### How it works:\n",
        "                1. **Upload a PDF** - Answers prioritize PDF content when relevant\n",
        "                2. **Ask questions** - System uses PDF, web search, and past conversations\n",
        "                3. **Long-term memory** - Remembers past conversations for context\n",
        "                4. **Multiple sources** - Combines PDF ğŸ“š, Web ğŸ”, and Memory ğŸ’­\n",
        "                \"\"\")\n",
        "\n",
        "            with gr.Column():\n",
        "                chat_interface = gr.ChatInterface(\n",
        "                    fn=chat_with_physics_bot,\n",
        "                    title=\"Chat with Physics Bot (with Memory)\",\n",
        "                    description=\"Ask questions about physics. I'll remember our past conversations and use multiple knowledge sources to help you!\"\n",
        "                )\n",
        "\n",
        "        # Load PDF when button clicked\n",
        "        load_btn.click(\n",
        "            fn=lambda pdf: store_pdf_in_db(pdf.name) if pdf else \"Please upload a PDF first\",\n",
        "            inputs=[pdf_input],\n",
        "            outputs=[load_status]\n",
        "        )\n",
        "\n",
        "        # Memory management buttons\n",
        "        clear_mem_btn.click(\n",
        "            fn=clear_memory,\n",
        "            outputs=[mem_status]\n",
        "        )\n",
        "\n",
        "        export_mem_btn.click(\n",
        "            fn=export_memory,\n",
        "            outputs=[mem_status]\n",
        "        )\n",
        "\n",
        "    demo.launch()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check for required API keys\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        print(\"âŒ OPENAI_API_KEY not found in environment variables\")\n",
        "        exit(1)\n",
        "\n",
        "    if not os.getenv(\"TAVILY_API_KEY\") and TAVILY_AVAILABLE:\n",
        "        print(\"âŒ TAVILY_API_KEY not found in environment variables\")\n",
        "        print(\"Please get your free API key from: https://app.tavily.com/\")\n",
        "        print(\"Continuing without web search functionality...\")\n",
        "\n",
        "    # Auto-load PDF if exists\n",
        "    pdf_files = [f for f in os.listdir('.') if f.endswith('.pdf')]\n",
        "    if pdf_files:\n",
        "        print(f\"Found PDF: {pdf_files[0]}\")\n",
        "        store_pdf_in_db(pdf_files[0])\n",
        "\n",
        "    print(\"ğŸ§  Long-term memory enabled! The bot will remember past conversations.\")\n",
        "    launch_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "iBiQBI3urYoG",
        "outputId": "025bc9aa-a22c-46dc-8d33-39de5a7caf31"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  Long-term memory enabled! The bot will remember past conversations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5ac8a1588692791c91.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5ac8a1588692791c91.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "\n",
        "def download_pdf_from_github(url, filename=\"PhysicsTeacher.pdf\"):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # ensure no errors\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "    return filename"
      ],
      "metadata": {
        "id": "bdR80YdLrucl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Correct GitHub raw URL\n",
        "    github_pdf_url = \"https://github.com/sumit1311singh/PhysicsTeacher-CurrentElectricity-Bot/raw/main/Current_Electricity.pdf\"\n",
        "\n",
        "    # Download PDF\n",
        "    pdf_file = download_pdf_from_github(github_pdf_url, \"Physics-PDFs\")\n",
        "    print(f\"Downloaded PDF: {pdf_file}\")\n",
        "\n",
        "    # Store in DB\n",
        "    store_pdf_in_db(pdf_file)\n",
        "\n",
        "    # Launch app\n",
        "    launch_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "id": "lQaq-x_CrzLd",
        "outputId": "24d4fec5-9320-4f2b-e1a7-a951a5f32831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded PDF: Physics-PDFs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79.3M/79.3M [00:00<00:00, 92.4MiB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://5eb52757e590cadfe9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5eb52757e590cadfe9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ofwGV-RJtYJE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}