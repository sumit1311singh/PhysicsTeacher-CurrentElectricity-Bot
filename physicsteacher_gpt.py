# -*- coding: utf-8 -*-
"""PhysicsTeacher-GPT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F9-A71DrU7B11AWULyDh_Dxx-0L-yYBD
"""

! pip install PyPDF2
! pip install chromadb
! pip install tavily

from openai import OpenAI
import gradio as gr
import os
import PyPDF2
import chromadb
from dotenv import load_dotenv
import requests
from urllib.parse import urlparse
import re
import json
from datetime import datetime
import uuid

os.environ["OPENAI_API_KEY"] = "" #Add your OPENAI API Key
load_dotenv()
from openai import OpenAI
client = OpenAI(api_key="")

# Initialize Tavily client with error handling
os.environ["TAVILY_API_KEY"] = "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"
try:
    from tavily import TavilyClient
    tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
    TAVILY_AVAILABLE = True
except ImportError:
    TAVILY_AVAILABLE = False
    print("âš ï¸  Tavily not installed. Web search functionality will be disabled.")
    print("ðŸ’¡ Run: pip install tavily-python")

# Initialize ChromaDB
chroma_client = chromadb.Client()
pdf_collection = chroma_client.create_collection(name="PhysicsPDFs")
memory_collection = chroma_client.create_collection(name="ConversationMemory")

def extract_text_from_pdf(pdf_path):
    """Extract text from PDF file"""
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
    return text

def chunk_text(text, chunk_size=1000):
    """Split text into chunks"""
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

def store_pdf_in_db(pdf_path):
    """Extract, chunk and store PDF in ChromaDB"""
    try:
        text = extract_text_from_pdf(pdf_path)
        chunks = chunk_text(text)

        # Clear existing data
        pdf_collection.delete(where={"source": {"$ne": ""}})  # Delete all documents

        # Add chunks to ChromaDB
        for i, chunk in enumerate(chunks):
            pdf_collection.add(
                documents=[chunk],
                metadatas=[{"source": pdf_path, "chunk_id": i}],
                ids=[f"chunk_{i}_{os.path.basename(pdf_path)}"]
            )
        return f"âœ… Successfully loaded {len(chunks)} chunks from {pdf_path}"
    except Exception as e:
        return f"âŒ Error: {str(e)}"

def get_relevant_context(query, n_results=3):
    """Retrieve relevant context from ChromaDB"""
    try:
        results = pdf_collection.query(
            query_texts=[query],
            n_results=n_results
        )
        return "\n\n".join(results['documents'][0]) if results['documents'] else ""
    except Exception as e:
        print(f"ChromaDB query error: {e}")
        return ""

def store_conversation_memory(user_message, bot_response, conversation_context=""):
    """Store conversation in long-term memory"""
    try:
        # Create a memory entry
        memory_id = str(uuid.uuid4())
        timestamp = datetime.now().isoformat()

        # Combine user message and bot response for semantic search
        memory_text = f"User: {user_message}\nBot: {bot_response}"

        if conversation_context:
            memory_text = f"Context: {conversation_context}\n{memory_text}"

        # Store in memory collection
        memory_collection.add(
            documents=[memory_text],
            metadatas=[{
                "type": "conversation",
                "timestamp": timestamp,
                "user_message": user_message,
                "bot_response": bot_response,
                "conversation_context": conversation_context
            }],
            ids=[memory_id]
        )

        return True
    except Exception as e:
        print(f"Memory storage error: {e}")
        return False

def get_relevant_memories(query, n_results=2):
    """Retrieve relevant past conversations from memory"""
    try:
        results = memory_collection.query(
            query_texts=[query],
            n_results=n_results
        )

        memories = []
        if results['documents']:
            for i, doc in enumerate(results['documents'][0]):
                metadata = results['metadatas'][0][i]
                memory_text = f"Previous conversation ({metadata['timestamp'][:10]}): {doc}"
                memories.append(memory_text)

        return "\n\n".join(memories) if memories else ""
    except Exception as e:
        print(f"Memory retrieval error: {e}")
        return ""

def get_conversation_summary(history):
    """Generate a summary of the current conversation context"""
    if not history:
        return ""

    # Take last few exchanges for context
    recent_history = history[-3:] if len(history) > 3 else history
    context_parts = []

    for exchange in recent_history:
        if isinstance(exchange, tuple) and len(exchange) == 2:
            user_msg, bot_msg = exchange
            context_parts.append(f"User: {user_msg}")
            context_parts.append(f"Bot: {bot_msg}")
        elif isinstance(exchange, list) and len(exchange) == 2:
            user_msg, bot_msg = exchange
            context_parts.append(f"User: {user_msg}")
            context_parts.append(f"Bot: {bot_msg}")

    return "\n".join(context_parts)

def is_pdf_context_relevant(pdf_context, query):
    """Check if PDF context is actually relevant to the query"""
    if not pdf_context or len(pdf_context.strip()) < 50:
        return False

    # Check for common irrelevant patterns in PDF context
    irrelevant_patterns = [
        "table of contents", "copyright", "abstract", "references",
        "appendix", "index", "acknowledg", "preface"
    ]

    pdf_lower = pdf_context.lower()
    query_lower = query.lower()

    # If PDF context contains many irrelevant sections, it's likely not useful
    irrelevant_count = sum(1 for pattern in irrelevant_patterns if pattern in pdf_lower)
    if irrelevant_count > 2:
        return False

    # Check if query terms actually appear in the context
    query_terms = query_lower.split()
    relevant_terms = sum(1 for term in query_terms if len(term) > 3 and term in pdf_lower)

    # If less than 30% of meaningful query terms are in context, consider it irrelevant
    meaningful_terms = [term for term in query_terms if len(term) > 3]
    if meaningful_terms and (relevant_terms / len(meaningful_terms)) < 0.3:
        return False

    return True

def search_with_tavily(query):
    """Search using Tavily and get content from first result"""
    if not TAVILY_AVAILABLE:
        return "Web search unavailable. Please install tavily-python: pip install tavily-python"

    try:
        # Search for the query
        search_response = tavily_client.search(
            query=query,
            search_depth="basic",
            max_results=3
        )

        if not search_response.get('results'):
            return "No search results found."

        # Get the first result
        first_result = search_response['results'][0]
        first_url = first_result.get('url', '')

        # Use Tavily's content if available
        content = first_result.get('content', '')

        if not content:
            content = f"Title: {first_result.get('title', 'N/A')}\nURL: {first_url}"

        # Check if it's a PDF
        if first_url.lower().endswith('.pdf'):
            return f"ðŸ“„ PDF Source: {first_url}\n\nThis appears to be a PDF document. Please visit the link to view the PDF: {first_url}"

        return f"ðŸ” Search Result from: {first_url}\n\n{content}"

    except Exception as e:
        return f"âŒ Search error: {str(e)}"

def chat_with_physics_bot(message, history):
    # Get relevant context from different sources
    pdf_context = get_relevant_context(message)
    memory_context = get_relevant_memories(message)
    conversation_context = get_conversation_summary(history)

    # Check if PDF context is actually relevant
    use_pdf = is_pdf_context_relevant(pdf_context, message)

    system_prompt = """You are a friendly Physics Teacher Bot with memory. Follow these rules:
    1. Use the provided context from PDFs, past conversations, and search results to answer questions accurately
    2. If the context doesn't fully answer the question, use your knowledge to provide a complete answer
    3. Reference past conversations when relevant to provide continuity
    4. Be educational and clear in your explanations
    5. Maintain context from the current conversation flow"""

    # Build context sections
    context_sections = []
    source_notes = []

    if use_pdf:
        context_sections.append(f"PDF Knowledge:\n{pdf_context}")
        source_notes.append("ðŸ“š PDF")

    if memory_context:
        context_sections.append(f"Past Conversations:\n{memory_context}")
        source_notes.append("ðŸ’­ Memory")

    if conversation_context:
        context_sections.append(f"Current Conversation:\n{conversation_context}")

    # If no PDF context is relevant, use web search
    if not use_pdf:
        search_results = search_with_tavily(message)
        context_sections.append(f"Web Search:\n{search_results}")
        source_notes.append("ðŸ” Web")

    # Combine all contexts
    combined_context = "\n\n".join(context_sections) if context_sections else "No specific context available."
    source_note = " + ".join(source_notes) if source_notes else "General knowledge"

    user_content = f"""Available Context:
{combined_context}

Question: {message}

Please provide a helpful answer based on the available context above."""

    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.2
        )

        bot_response = response.choices[0].message.content

        # Store this conversation in long-term memory
        store_conversation_memory(message, bot_response, conversation_context)

        return f"ðŸ”® Sources: {source_note}\n\n{bot_response}"

    except Exception as e:
        return f"âŒ Error generating response: {str(e)}"

def clear_memory():
    """Clear all conversation memory"""
    try:
        memory_collection.delete(where={"type": {"$ne": ""}})
        return "âœ… Conversation memory cleared successfully!"
    except Exception as e:
        return f"âŒ Error clearing memory: {str(e)}"

def export_memory():
    """Export conversation memory as JSON"""
    try:
        results = memory_collection.get()
        memories = []

        for i, doc in enumerate(results['documents']):
            memories.append({
                "content": doc,
                "metadata": results['metadatas'][i],
                "id": results['ids'][i]
            })

        filename = f"conversation_memory_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w') as f:
            json.dump(memories, f, indent=2)

        return f"âœ… Memory exported to {filename}"
    except Exception as e:
        return f"âŒ Error exporting memory: {str(e)}"

def load_pdf_and_chat(pdf_file, message, history):
    # First store the PDF
    result_msg = store_pdf_in_db(pdf_file.name)

    # Then get response
    response = chat_with_physics_bot(message, history)

    return result_msg, response

def launch_app():
    with gr.Blocks() as demo:
        gr.Markdown("# âš›ï¸ Physics Teacher Chat with PDF RAG + Web Search + Memory")

        with gr.Row():
            with gr.Column():
                pdf_input = gr.File(label="Upload PDF", file_types=[".pdf"])
                load_btn = gr.Button("Load PDF into Knowledge Base")
                load_status = gr.Textbox(label="Load Status", interactive=False)

                gr.Markdown("### Memory Management")
                with gr.Row():
                    clear_mem_btn = gr.Button("ðŸ§¹ Clear Memory")
                    export_mem_btn = gr.Button("ðŸ’¾ Export Memory")
                mem_status = gr.Textbox(label="Memory Status", interactive=False)

                gr.Markdown("""
                ### How it works:
                1. **Upload a PDF** - Answers prioritize PDF content when relevant
                2. **Ask questions** - System uses PDF, web search, and past conversations
                3. **Long-term memory** - Remembers past conversations for context
                4. **Multiple sources** - Combines PDF ðŸ“š, Web ðŸ”, and Memory ðŸ’­
                """)

            with gr.Column():
                chat_interface = gr.ChatInterface(
                    fn=chat_with_physics_bot,
                    title="Chat with Physics Bot (with Memory)",
                    description="Ask questions about physics. I'll remember our past conversations and use multiple knowledge sources to help you!"
                )

        # Load PDF when button clicked
        load_btn.click(
            fn=lambda pdf: store_pdf_in_db(pdf.name) if pdf else "Please upload a PDF first",
            inputs=[pdf_input],
            outputs=[load_status]
        )

        # Memory management buttons
        clear_mem_btn.click(
            fn=clear_memory,
            outputs=[mem_status]
        )

        export_mem_btn.click(
            fn=export_memory,
            outputs=[mem_status]
        )

    demo.launch()

if __name__ == "__main__":
    # Check for required API keys
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEY not found in environment variables")
        exit(1)

    if not os.getenv("TAVILY_API_KEY") and TAVILY_AVAILABLE:
        print("âŒ TAVILY_API_KEY not found in environment variables")
        print("Please get your free API key from: https://app.tavily.com/")
        print("Continuing without web search functionality...")

    # Auto-load PDF if exists
    pdf_files = [f for f in os.listdir('.') if f.endswith('.pdf')]
    if pdf_files:
        print(f"Found PDF: {pdf_files[0]}")
        store_pdf_in_db(pdf_files[0])

    print("ðŸ§  Long-term memory enabled! The bot will remember past conversations.")
    launch_app()

import requests
import os

def download_pdf_from_github(url, filename="PhysicsTeacher.pdf"):
    response = requests.get(url)
    response.raise_for_status()  # ensure no errors
    with open(filename, "wb") as f:
        f.write(response.content)
    return filename

if __name__ == "__main__":
    # Correct GitHub raw URL
    github_pdf_url = "https://github.com/sumit1311singh/PhysicsTeacher-CurrentElectricity-Bot/raw/main/Current_Electricity.pdf"

    # Download PDF
    pdf_file = download_pdf_from_github(github_pdf_url, "Physics-PDFs")
    print(f"Downloaded PDF: {pdf_file}")

    # Store in DB
    store_pdf_in_db(pdf_file)

    # Launch app
    launch_app()

